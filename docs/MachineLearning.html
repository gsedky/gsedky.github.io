<!DOCTYPE HTML>

<html>
	<head>
		<title>ML</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a class="logo">Projects</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">Projects</a></li>
							<li><a href="Teaching.html">Teaching</a></li>
							<li><a href="About.html">About</a></li>
						</ul>
					<ul class="icons">
						<li><a href="https://twitter.com/GirguisS" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="https://github.com/gsedky" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						<li><a href="https://www.youtube.com/channel/UCHuBhatUGwUYrannYb7wIFA" class="icon brands fa-youtube"><span class="label">Youtube</span></a></li>

					</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<h1>Classification via Machine Learning</h1>

								</header>


              <section>
              <h3>Introduction</h3>
								<p>This project was a part of a Statistical Pattern Recognition course taught by <a href="https://scholar.google.com/citations?user=L60tuywAAAAJ&hl=en">Dr. Rama Chellappa</a> at the University of Maryland, College Park. In this project, we were tasked to classify the <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a> data set successfully using various machine learning alrgorithms. Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. The classification algorithms included
									<ul>
										<li>Statistical gaussian Bayesean estimation</li>
										<li>Nearest-Neighborhood</li>
										<li>linear and non-linear, kernel Support Vector Machines</li>
										<li>Convolutional Neural Network</li>
									</u1>

									<p>
										<center><img src="images/ML/fashion-mnist-sprite.png" alt="" /></center>

							</section>
              <section>
              <h3>Statistical gaussian Bayesean estimation</h3>
              <p>In this method, each pixel had a grayscale value ranging from 0 (black) to 255 (white). The pixel values of a picture is columnated to create a vector \(\boldsymbol{x}\) that contains 784 elements. It is assumed that the conditional probability density function of a vector given a certain class (clothing item) \(\omega_i\), \(P(\boldsymbol{x}|\omega_i)\) follows a gaussian distribution with with a mean \(\boldsymbol{\mu}\) and a variance \(\Sigma\). In addition,
								the prior class probability \(P(\omega_i)\) of all the classes were assumed to be equal during training and testing. This means that before seeing the picture, the probability that the picture belongs to a certain class of clothing items is the same for all clothing items. Under this assumption, the Bayesian estimation rule reduces to assigning a state vector (picture) to the class (clothing item)
								that maximizes the conditional probability
								$$P(\boldsymbol{x}|\omega_i) = \frac{1}{((2\pi)^d |\Sigma|)^{1/2}}\exp\left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\text{T}}\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right)$$
								where \(d=784\) is the length of the state vectors. This is equivalent to maximizing

								$$\log\left(P(\boldsymbol{x}|\omega_i)\right) = -\frac{1}{2}\log(|\Sigma|)- \frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\text{T}}\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu}).$$
								The sample mean and covariances for each class are
								$$\boldsymbol{\hat{\mu}}_i = \frac{1}{n}\sum_{j=1}^{n}\boldsymbol{x}_j,$$

								$$\hat{\Sigma}_i = \frac{1}{n}\sum_{j=1}^{n}(\boldsymbol{x}_j-\boldsymbol{\hat{\mu}}_i)(\boldsymbol{x}_j-\boldsymbol{\hat{\mu}}_i)^{\text{T}}.$$

								A training script was used on the training set comprised of 60,000 images to determine \(\boldsymbol{\hat{\mu}}_i\) and \(\hat{\Sigma}_i\) of every class and a testing script in turn used these values and the Bayesian classification algorithm to classify the test images.
								Since the covariance matrices are large and some are singular, special computational techniques were used to ensure good results. A pseudo inverse operation was used to inverse \(|\Sigma_i|\) and small multiple of the identity was added to \(|\Sigma_i|\) before obtaining its determinant. <b>The classification error on the testing set came to about 35 %.</b>
							</p>
							</section>

							<section>
								<h3>Nearest Neighborhood classifier</h3>
								<p>In this method, the image state vectors are live in \(\mathbb{R}^{784}\). In this space, each image in the training set has a state vector that constitutes a prototype. To classify an image in the testing set, the euclidiean distance between the testing image and all the training images is found.
									$$d(\boldsymbol{x}_\text{test},\boldsymbol{x}_\text{train})=\sqrt{(x_{1,\text{test}}-x_{1,\text{test}})^2 + (x_{2,\text{test}}-x_{2,\text{test}})^2 + .... + (x_{784,\text{test}}-x_{784,\text{test}})^2}$$
									Then, the testing image is assigned to the class of the training image that is closest to it. <b>The classification error on the testing set came to about 15 %.</b>
							</p>
							</section>

             <h3>Principal component analysis (PCA) and linear discriminant analysis (LDA)</h3>
						 <p>PCA and LDA were used to test whether we can improve the estimates of the estimators mentioned above if we lowered the dimensionality of the state vectors of the images.</p>
             <p>
							 <h4>PCA</h4>
							 Principal component analysis was used to reduce the dimensionality of the state vectors while maintaining most of the information captured by the lower dimensional vectors. The low-order state vectors were then fed into the Bayesian and Nearest Neighborhood classifier. The following figures demonstrate the percentage error in classification based on the number of dimensions kept. <b>The minimum classification error came to about 20.5% for 50 kept vector components in the case of the Bayesian classifier and 14.7 % for 150 vector components kept in the case of the Nearest Neighborhood classier.</b>
							 <figure>
							   <center><img src="images/ML/ML_PCA.png" alt="" /></center>
							   <center><figcaption>Bayesian classifier</figcaption><center>
							 </figure>
							 <figure>
							   <center><img src="images/ML/ML_PCA.png" alt="" /></center>
							   <center><figcaption>Nearest Neighborhood classier</figcaption><center>
							 </figure>

							 <h4>LDA</h4>
							 Linear discriminant analysis was used to reduce the image state vectors from \(\mathbb{R}^{784}\) to \(\mathbb{R}^{M-1}\) where \(M=10\) is the number of classes. The built-in MATLAB function was used for this purpose. Again, the low-order state vectors were then fed into the Bayesian and Nearest Neighborhood classifier. <b>The minimum classification error came to about 25.4% in the case of the Bayesian classifier and 28% in the case of the Nearest Neighborhood classier.</b>

						</p>
					</div>

					<!-- Footer -->
						<footer id="footer">
							<section class="split contact">
								<section>
									<h3>Email</h3>
									<p><a href="#">sedky.girguis@gmail.com</a></p>
								</section>
							</section>
						</footer>


			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	</body>
</html>
